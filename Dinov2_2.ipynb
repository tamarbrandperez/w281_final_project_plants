{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f660b98c-d7f4-4d23-9f46-3af6805f1cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to remove the model from my computer when done, it should be stored in ~/.cache/huggingface/transformers/:\n",
    "rm -rf ~/.cache/huggingface/transformers/facebook/dinov2-large\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6852381b-2055-45f4-82ac-eb8e3a02d93f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.17.2-cp311-cp311-macosx_10_13_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/site-packages (from torchvision) (1.24.3)\n",
      "Collecting torch==2.2.2 (from torchvision)\n",
      "  Downloading torch-2.2.2-cp311-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/site-packages (from torchvision) (9.5.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from torch==2.2.2->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/site-packages (from torch==2.2.2->torchvision) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/site-packages (from torch==2.2.2->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/site-packages (from torch==2.2.2->torchvision) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/site-packages (from torch==2.2.2->torchvision) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/site-packages (from torch==2.2.2->torchvision) (2024.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/site-packages (from jinja2->torch==2.2.2->torchvision) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/site-packages (from sympy->torch==2.2.2->torchvision) (1.3.0)\n",
      "Downloading torchvision-0.17.2-cp311-cp311-macosx_10_13_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.2.2-cp311-none-macosx_10_9_x86_64.whl (150.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.8/150.8 MB\u001b[0m \u001b[31m775.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:06\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch, torchvision\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.2.0\n",
      "    Uninstalling torch-2.2.0:\n",
      "      Successfully uninstalled torch-2.2.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchtext 0.17.0 requires torch==2.2.0, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-2.2.2 torchvision-0.17.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04d73e87-b163-4250-b2f5-20c743666d99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torchvision and transforms imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "print(\"Torchvision and transforms imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9637e1a2-4bc8-4711-8d6f-7a88240efbd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import csv\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7c35740-5f50-427a-a412-e577fc9f06aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preprocessor_config.json: 100%|█████████████████| 436/436 [00:00<00:00, 758kB/s]\n",
      "config.json: 100%|█████████████████████████████| 549/549 [00:00<00:00, 1.41MB/s]\n",
      "model.safetensors: 100%|███████████████████| 1.22G/1.22G [01:07<00:00, 18.0MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DINOv2 model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the DINO model and processor\n",
    "processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-large\")\n",
    "dinov2_model = AutoModel.from_pretrained(\"facebook/dinov2-large\").to(device)\n",
    "\n",
    "print(\"DINOv2 model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a82c1b7-d740-49a8-a913-2a0ff635108f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/562 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 32 images in 18.81 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                        | 1/562 [01:19<12:22:57, 79.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 60.63 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 21.71 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                       | 2/562 [02:38<12:17:36, 79.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 56.99 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 21.93 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏                                       | 3/562 [03:58<12:21:58, 79.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 58.42 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 23.71 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▎                                       | 4/562 [05:20<12:29:11, 80.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 58.23 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 22.84 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▎                                       | 5/562 [06:38<12:17:49, 79.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.70 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 20.16 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                       | 6/562 [07:51<11:58:04, 77.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.43 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 21.34 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                       | 7/562 [09:06<11:49:22, 76.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.67 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 21.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                       | 8/562 [10:20<11:40:26, 75.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.84 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.62 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▋                                       | 9/562 [11:33<11:29:55, 74.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.00 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 21.81 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▋                                      | 10/562 [12:48<11:29:25, 74.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.27 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 20.65 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▊                                      | 11/562 [14:03<11:26:56, 74.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.82 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 27.48 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▊                                      | 12/562 [15:24<11:44:03, 76.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.88 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 22.76 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▉                                      | 13/562 [16:41<11:42:04, 76.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.75 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 22.30 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▉                                      | 14/562 [17:56<11:37:58, 76.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.37 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 23.23 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█                                      | 15/562 [19:13<11:37:11, 76.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.33 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 23.26 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█                                      | 16/562 [20:31<11:40:22, 76.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.80 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 22.08 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█▏                                     | 17/562 [21:46<11:34:33, 76.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.18 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 23.79 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█▏                                     | 18/562 [23:04<11:36:03, 76.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.67 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 22.28 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█▎                                     | 19/562 [24:19<11:30:41, 76.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.94 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 31.24 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█▍                                     | 20/562 [25:45<11:54:42, 79.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.34 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 23.74 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█▍                                     | 21/562 [27:02<11:49:29, 78.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.87 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 22.89 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█▌                                     | 22/562 [28:21<11:48:53, 78.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 55.99 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 22.45 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█▌                                     | 23/562 [29:39<11:44:28, 78.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 55.09 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 25.66 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█▋                                     | 24/562 [30:59<11:46:25, 78.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.93 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 23.40 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█▋                                     | 25/562 [32:16<11:42:51, 78.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.50 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 41.45 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|█▊                                     | 26/562 [33:52<12:26:36, 83.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.82 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 24.41 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|█▊                                     | 27/562 [35:10<12:11:55, 82.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.15 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.93 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|█▉                                     | 28/562 [36:25<11:50:32, 79.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.61 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.24 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|██                                     | 29/562 [37:39<11:34:07, 78.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.89 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 20.40 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|██                                     | 30/562 [38:55<11:27:26, 77.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 55.66 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 21.57 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|██▏                                    | 31/562 [40:11<11:20:49, 76.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.90 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 21.92 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|██▏                                    | 32/562 [41:26<11:15:37, 76.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.47 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 23.34 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|██▎                                    | 33/562 [42:44<11:16:41, 76.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.97 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 35.26 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|██▎                                    | 34/562 [44:12<11:46:02, 80.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.04 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 23.99 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|██▍                                    | 35/562 [45:29<11:37:15, 79.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.34 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 21.71 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|██▍                                    | 36/562 [46:44<11:23:48, 78.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.01 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 26.27 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██▌                                    | 37/562 [48:06<11:31:33, 79.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 55.08 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 20.25 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██▋                                    | 38/562 [49:22<11:23:21, 78.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 56.08 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.31 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██▋                                    | 39/562 [50:37<11:12:22, 77.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 55.20 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.01 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██▊                                    | 40/562 [51:52<11:06:18, 76.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 56.27 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 20.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██▊                                    | 41/562 [53:09<11:05:33, 76.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 56.73 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 20.48 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██▉                                    | 42/562 [54:25<11:03:13, 76.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 55.73 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.29 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██▉                                    | 43/562 [55:40<10:58:45, 76.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 55.97 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 20.29 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███                                    | 44/562 [56:55<10:52:41, 75.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.98 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 18.89 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███                                    | 45/562 [58:11<10:53:47, 75.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 57.55 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.68 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███▏                                   | 46/562 [59:26<10:49:30, 75.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.98 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.64 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███                                  | 47/562 [1:00:40<10:45:10, 75.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.66 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.17 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███▏                                 | 48/562 [1:01:56<10:45:38, 75.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 56.62 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 17.66 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███▏                                 | 49/562 [1:03:07<10:32:25, 73.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.01 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.61 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███▎                                 | 50/562 [1:04:19<10:27:36, 73.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.90 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 21.49 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███▎                                 | 51/562 [1:05:34<10:28:41, 73.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.93 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 21.45 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███▍                                 | 52/562 [1:06:48<10:28:25, 73.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.69 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 22.40 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███▍                                 | 53/562 [1:08:03<10:30:31, 74.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.80 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 21.55 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███▌                                 | 54/562 [1:09:17<10:28:50, 74.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.57 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 22.22 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███▌                                 | 55/562 [1:10:36<10:39:28, 75.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 56.67 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 24.37 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███▋                                 | 56/562 [1:11:58<10:53:09, 77.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 57.15 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 22.09 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███▊                                 | 57/562 [1:13:18<10:58:07, 78.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 57.77 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 22.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███▊                                 | 58/562 [1:14:38<11:03:15, 78.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 58.50 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 21.14 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███▉                                 | 59/562 [1:15:59<11:04:44, 79.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 58.87 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 22.23 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|███▉                                 | 60/562 [1:17:19<11:07:10, 79.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 58.50 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 22.98 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████                                 | 61/562 [1:18:41<11:10:15, 80.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 58.46 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 22.20 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████                                 | 62/562 [1:20:01<11:09:35, 80.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 58.29 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 21.76 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████▏                                | 63/562 [1:21:22<11:08:12, 80.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 58.52 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 21.81 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████▏                                | 64/562 [1:22:38<10:55:42, 79.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.02 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 21.37 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|████▎                                | 65/562 [1:23:52<10:44:09, 77.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.47 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 21.06 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|████▎                                | 66/562 [1:25:07<10:33:58, 76.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.08 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 20.77 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|████▍                                | 67/562 [1:26:21<10:26:13, 75.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.26 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 21.91 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|████▍                                | 68/562 [1:27:36<10:23:02, 75.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.18 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 20.47 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|████▌                                | 69/562 [1:28:49<10:16:28, 75.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.00 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 20.58 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|████▌                                | 70/562 [1:30:03<10:11:52, 74.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.04 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 20.17 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|████▋                                | 71/562 [1:31:16<10:07:26, 74.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.10 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 20.28 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|████▋                                | 72/562 [1:32:29<10:03:16, 73.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.71 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 23.83 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|████▊                                | 73/562 [1:33:46<10:08:35, 74.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.68 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 24.75 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|████▊                                | 74/562 [1:35:04<10:15:41, 75.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.28 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 21.44 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|████▉                                | 75/562 [1:36:20<10:15:58, 75.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.83 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.75 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█████                                | 76/562 [1:37:34<10:10:14, 75.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.25 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 20.60 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█████                                | 77/562 [1:38:49<10:06:54, 75.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.82 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.09 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█████▏                               | 78/562 [1:40:02<10:01:25, 74.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.20 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.84 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█████▎                                | 79/562 [1:41:16<9:58:12, 74.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.83 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 20.12 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█████▍                                | 80/562 [1:42:31<9:58:39, 74.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.86 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 20.61 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█████▍                                | 81/562 [1:43:46<9:57:35, 74.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.94 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 21.15 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████▌                                | 82/562 [1:45:00<9:56:11, 74.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.27 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 18.70 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████▌                                | 83/562 [1:46:12<9:47:58, 73.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.85 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 20.15 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████▋                                | 84/562 [1:47:24<9:43:07, 73.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 51.95 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 20.73 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████▋                                | 85/562 [1:48:37<9:42:01, 73.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.46 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 21.39 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████▊                                | 86/562 [1:49:50<9:40:30, 73.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 51.67 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 21.63 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████▉                                | 87/562 [1:51:03<9:39:08, 73.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 51.43 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 21.69 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█████▉                                | 88/562 [1:52:16<9:38:00, 73.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 51.44 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 20.44 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|██████                                | 89/562 [1:53:29<9:34:50, 72.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 51.85 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 22.88 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|██████                                | 90/562 [1:54:44<9:37:58, 73.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 51.82 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 21.80 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|██████▏                               | 91/562 [1:55:57<9:37:45, 73.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.06 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 22.38 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|██████▏                               | 92/562 [1:57:12<9:38:52, 73.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.17 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.98 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|██████▎                               | 93/562 [1:58:26<9:38:48, 74.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.37 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 22.34 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|██████▎                               | 94/562 [1:59:42<9:40:05, 74.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.74 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 21.85 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|██████▍                               | 95/562 [2:00:56<9:39:20, 74.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.68 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.30 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|██████▍                               | 96/562 [2:02:10<9:36:30, 74.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.38 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.66 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|██████▌                               | 97/562 [2:03:22<9:31:15, 73.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.79 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 18.73 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|██████▋                               | 98/562 [2:04:34<9:24:32, 73.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.56 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.27 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|██████▋                               | 99/562 [2:05:46<9:21:57, 72.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.08 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 17.90 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|██████▌                              | 100/562 [2:06:57<9:16:01, 72.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.84 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 18.83 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|██████▋                              | 101/562 [2:08:09<9:14:05, 72.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.01 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 18.89 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|██████▋                              | 102/562 [2:09:21<9:11:58, 72.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.78 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 18.46 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|██████▊                              | 103/562 [2:10:32<9:09:41, 71.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.04 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.58 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|██████▊                              | 104/562 [2:11:45<9:09:57, 72.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.87 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.11 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|██████▉                              | 105/562 [2:12:57<9:08:44, 72.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.89 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 18.84 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|██████▉                              | 106/562 [2:14:08<9:07:11, 72.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.01 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.51 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|███████                              | 107/562 [2:15:21<9:06:54, 72.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.85 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 21.92 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|███████                              | 108/562 [2:16:36<9:12:52, 73.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.28 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.45 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|███████▏                             | 109/562 [2:17:49<9:10:31, 72.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.05 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 20.17 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████▏                             | 110/562 [2:19:02<9:09:37, 72.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.83 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 18.76 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████▎                             | 111/562 [2:20:13<9:05:38, 72.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.91 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.18 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████▎                             | 112/562 [2:21:26<9:03:39, 72.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.01 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 18.24 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████▍                             | 113/562 [2:22:37<9:00:09, 72.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.18 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 20.06 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████▌                             | 114/562 [2:23:50<9:00:47, 72.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.88 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.25 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████▌                             | 115/562 [2:25:02<8:59:15, 72.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.97 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.09 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|███████▋                             | 116/562 [2:26:15<8:57:59, 72.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.21 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.03 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|███████▋                             | 117/562 [2:27:27<8:55:37, 72.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.78 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 18.54 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|███████▊                             | 118/562 [2:28:38<8:53:22, 72.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.15 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.78 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|███████▊                             | 119/562 [2:29:52<8:54:27, 72.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.25 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 20.04 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|███████▉                             | 120/562 [2:31:05<8:55:24, 72.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.29 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 20.12 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|███████▉                             | 121/562 [2:32:18<8:55:42, 72.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.18 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.86 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|████████                             | 122/562 [2:33:31<8:54:54, 72.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.17 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.14 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|████████                             | 123/562 [2:34:44<8:52:39, 72.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.27 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 20.36 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|████████▏                            | 124/562 [2:35:58<8:54:15, 73.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.67 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.29 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|████████▏                            | 125/562 [2:37:10<8:51:17, 72.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.04 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.84 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|████████▎                            | 126/562 [2:38:23<8:50:28, 73.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.22 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|████████▎                            | 127/562 [2:39:36<8:48:02, 72.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.18 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.37 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|████████▍                            | 128/562 [2:40:49<8:47:48, 72.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.88 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.09 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|████████▍                            | 129/562 [2:42:02<8:46:03, 72.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.59 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 21.48 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|████████▌                            | 130/562 [2:43:16<8:48:17, 73.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.95 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 20.53 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|████████▌                            | 131/562 [2:44:30<8:47:16, 73.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.90 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.61 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|████████▋                            | 132/562 [2:45:42<8:43:40, 73.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.64 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.75 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|████████▊                            | 133/562 [2:46:55<8:41:16, 72.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.73 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 18.68 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|████████▊                            | 134/562 [2:48:07<8:39:20, 72.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.82 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.29 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|████████▉                            | 135/562 [2:49:19<8:36:19, 72.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.63 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 20.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|████████▉                            | 136/562 [2:50:32<8:35:41, 72.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.78 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 21.29 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|█████████                            | 137/562 [2:51:46<8:37:24, 73.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.67 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.27 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████                            | 138/562 [2:52:58<8:33:40, 72.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.55 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 20.50 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████▏                           | 139/562 [2:54:11<8:33:23, 72.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.58 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 20.01 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████▏                           | 140/562 [2:55:24<8:31:51, 72.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.60 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 18.61 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████▎                           | 141/562 [2:56:35<8:27:55, 72.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.83 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 18.98 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████▎                           | 142/562 [2:57:48<8:27:13, 72.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.60 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 20.75 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████▍                           | 143/562 [2:59:02<8:29:19, 72.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.24 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.90 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|█████████▍                           | 144/562 [3:00:16<8:30:18, 73.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.04 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.28 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|█████████▌                           | 145/562 [3:01:30<8:30:07, 73.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.42 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.08 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|█████████▌                           | 146/562 [3:02:43<8:28:43, 73.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.19 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.17 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|█████████▋                           | 147/562 [3:03:55<8:25:06, 73.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.01 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 18.26 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|█████████▋                           | 148/562 [3:05:07<8:21:56, 72.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.76 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.65 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|█████████▊                           | 149/562 [3:06:21<8:22:10, 72.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.73 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 18.95 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|█████████▉                           | 150/562 [3:07:35<8:23:52, 73.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 55.37 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.87 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|█████████▉                           | 151/562 [3:08:49<8:23:58, 73.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.10 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 18.64 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██████████                           | 152/562 [3:10:03<8:22:36, 73.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.82 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 18.89 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██████████                           | 153/562 [3:11:15<8:20:11, 73.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.05 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.73 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██████████▏                          | 154/562 [3:12:30<8:20:16, 73.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.25 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.51 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██████████▏                          | 155/562 [3:13:43<8:18:09, 73.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.58 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.09 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██████████▎                          | 156/562 [3:14:56<8:17:31, 73.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.60 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 20.62 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██████████▎                          | 157/562 [3:16:12<8:20:31, 74.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.95 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 18.69 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██████████▍                          | 158/562 [3:17:25<8:17:41, 73.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.63 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 20.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██████████▍                          | 159/562 [3:18:40<8:17:12, 74.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.03 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 20.64 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██████████▌                          | 160/562 [3:19:56<8:20:26, 74.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 55.57 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.18 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██████████▌                          | 161/562 [3:21:10<8:18:32, 74.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 55.15 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.89 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██████████▋                          | 162/562 [3:22:24<8:16:21, 74.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.20 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 20.34 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██████████▋                          | 163/562 [3:23:39<8:15:06, 74.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.07 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 20.44 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██████████▊                          | 164/562 [3:24:53<8:13:56, 74.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.00 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 22.43 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██████████▊                          | 165/562 [3:26:10<8:16:56, 75.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.14 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 18.62 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██████████▉                          | 166/562 [3:27:24<8:12:51, 74.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 55.00 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 20.64 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██████████▉                          | 167/562 [3:28:40<8:14:18, 75.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 55.35 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 18.70 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████                          | 168/562 [3:29:54<8:11:05, 74.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 55.36 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 18.63 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████▏                         | 169/562 [3:31:07<8:07:02, 74.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.70 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 33.06 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████▏                         | 170/562 [3:32:35<8:31:59, 78.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.62 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 18.34 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████▎                         | 171/562 [3:33:48<8:21:29, 76.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 55.29 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.32 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███████████▎                         | 172/562 [3:35:04<8:16:50, 76.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 55.88 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 18.28 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███████████▍                         | 173/562 [3:36:19<8:12:29, 75.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 56.52 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 19.30 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███████████▍                         | 174/562 [3:37:33<8:08:17, 75.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 55.12 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 18.14 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███████████▌                         | 175/562 [3:38:46<8:03:00, 74.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 55.25 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 18.06 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███████████▌                         | 176/562 [3:40:01<8:01:30, 74.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 56.67 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 18.56 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███████████▋                         | 177/562 [3:41:16<8:00:17, 74.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 56.28 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 16.80 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███████████▋                         | 178/562 [3:42:29<7:55:51, 74.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 56.36 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 17.27 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███████████▊                         | 179/562 [3:43:41<7:50:18, 73.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.81 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 16.56 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███████████▊                         | 180/562 [3:44:53<7:45:41, 73.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 55.30 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 16.06 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███████████▉                         | 181/562 [3:46:06<7:44:00, 73.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 56.78 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 15.76 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███████████▉                         | 182/562 [3:47:19<7:42:24, 73.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 57.09 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 16.29 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████                         | 183/562 [3:48:34<7:44:20, 73.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 58.35 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 16.62 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████                         | 184/562 [3:49:45<7:38:13, 72.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.27 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 17.81 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████▏                        | 185/562 [3:50:56<7:34:10, 72.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.39 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 16.62 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████▏                        | 186/562 [3:52:06<7:28:16, 71.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.15 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 15.93 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████▎                        | 187/562 [3:53:15<7:22:19, 70.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.04 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 16.95 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████▍                        | 188/562 [3:54:25<7:20:14, 70.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.30 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 17.16 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|████████████▍                        | 189/562 [3:55:35<7:17:56, 70.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.84 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 18.32 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|████████████▌                        | 190/562 [3:56:46<7:18:34, 70.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.07 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 17.71 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|████████████▌                        | 191/562 [3:57:59<7:20:29, 71.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.66 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 18.30 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|████████████▋                        | 192/562 [3:59:11<7:20:55, 71.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.78 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 17.16 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|████████████▋                        | 193/562 [4:00:21<7:16:28, 70.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.52 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 17.61 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|████████████▊                        | 194/562 [4:01:31<7:14:42, 70.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 52.98 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 17.41 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|████████████▊                        | 195/562 [4:02:42<7:12:58, 70.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.12 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 17.94 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|████████████▉                        | 196/562 [4:03:54<7:14:18, 71.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.19 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 17.05 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|████████████▉                        | 197/562 [4:05:05<7:12:06, 71.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 53.53 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 17.72 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|█████████████                        | 198/562 [4:06:17<7:13:31, 71.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.69 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 18.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|█████████████                        | 199/562 [4:07:30<7:14:48, 71.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.60 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 16.06 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|█████████████▏                       | 200/562 [4:08:41<7:11:30, 71.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.60 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 16.77 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|█████████████▏                       | 201/562 [4:09:52<7:10:51, 71.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.99 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 16.80 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|█████████████▎                       | 202/562 [4:11:04<7:09:17, 71.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.54 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 17.48 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|█████████████▎                       | 203/562 [4:12:16<7:08:32, 71.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.27 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 16.59 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|█████████████▍                       | 204/562 [4:13:26<7:05:42, 71.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and inferred 32 images in 54.06 seconds\n",
      "torch.Size([32, 1024])\n",
      "Loaded 32 images in 17.11 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|█████████████▍                       | 204/562 [4:13:50<7:25:27, 74.66s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m batch_files \u001b[38;5;241m=\u001b[39m image_files[i:i\u001b[38;5;241m+\u001b[39mBATCH_SIZE]\n\u001b[1;32m     44\u001b[0m batch_paths \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path_to_data_folder, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m batch_files]\n\u001b[0;32m---> 46\u001b[0m batch_features \u001b[38;5;241m=\u001b[39m \u001b[43mget_batch_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_paths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m all_features\u001b[38;5;241m.\u001b[39mextend(batch_features\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     49\u001b[0m image_names\u001b[38;5;241m.\u001b[39mextend(batch_files)\n",
      "Cell \u001b[0;32mIn[8], line 25\u001b[0m, in \u001b[0;36mget_batch_features\u001b[0;34m(image_paths)\u001b[0m\n\u001b[1;32m     23\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(images\u001b[38;5;241m=\u001b[39mimages, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 25\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdinov2_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m infer_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     27\u001b[0m infer_time \u001b[38;5;241m=\u001b[39m infer_end \u001b[38;5;241m-\u001b[39m infer_start\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/models/dinov2/modeling_dinov2.py:639\u001b[0m, in \u001b[0;36mDinov2Model.forward\u001b[0;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    635\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    637\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(pixel_values, bool_masked_pos\u001b[38;5;241m=\u001b[39mbool_masked_pos)\n\u001b[0;32m--> 639\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    646\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    647\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm(sequence_output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/models/dinov2/modeling_dinov2.py:459\u001b[0m, in \u001b[0;36mDinov2Encoder.forward\u001b[0;34m(self, hidden_states, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    452\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    453\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    454\u001b[0m         hidden_states,\n\u001b[1;32m    455\u001b[0m         layer_head_mask,\n\u001b[1;32m    456\u001b[0m         output_attentions,\n\u001b[1;32m    457\u001b[0m     )\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 459\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    461\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/models/dinov2/modeling_dinov2.py:400\u001b[0m, in \u001b[0;36mDinov2Layer.forward\u001b[0;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    396\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    397\u001b[0m     head_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    398\u001b[0m     output_attentions: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    399\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor], Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[0;32m--> 400\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# in Dinov2, layernorm is applied before self-attention\u001b[39;49;00m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    407\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_scale1(attention_output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/models/dinov2/modeling_dinov2.py:287\u001b[0m, in \u001b[0;36mDinov2Attention.forward\u001b[0;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    281\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    282\u001b[0m     head_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    283\u001b[0m     output_attentions: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    284\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor], Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[1;32m    285\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(hidden_states, head_mask, output_attentions)\n\u001b[0;32m--> 287\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/models/dinov2/modeling_dinov2.py:247\u001b[0m, in \u001b[0;36mDinov2SelfOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 247\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "dinov2_model.eval()\n",
    "\n",
    "# Path to your image folder\n",
    "#path_to_data_folder = \"/content/drive/MyDrive/w281_Final_Project_Leaf_Disease_Classification_Computer_Vision/Data/train_images\"\n",
    "path_to_data_folder = \"/Users/tamarbrand-perez/Library/CloudStorage/GoogleDrive-terez@berkeley.edu/My Drive/w281_Final_Project_Leaf_Disease_Classification_Computer_Vision/Data/train_images\"\n",
    "\n",
    "# Batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Function to process a batch of images and get their features\n",
    "def get_batch_features(image_paths):\n",
    "    # Measure image loading time\n",
    "    load_start = time.time()\n",
    "    images = [Image.open(path).convert('RGB') for path in image_paths]\n",
    "    load_end = time.time()\n",
    "    load_time = load_end - load_start\n",
    "    print(f\"Loaded {len(image_paths)} images in {load_time:.2f} seconds\")\n",
    "\n",
    "    # Measure preprocessing and inference time\n",
    "    infer_start = time.time()\n",
    "    inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = dinov2_model(**inputs)\n",
    "    infer_end = time.time()\n",
    "    infer_time = infer_end - infer_start\n",
    "    print(f\"Preprocessed and inferred {len(image_paths)} images in {infer_time:.2f} seconds\")\n",
    "    print(outputs[0][:, 0, :].shape)\n",
    "    return outputs[0][:, 0, :]\n",
    "\n",
    "# Get list of image files\n",
    "image_files = [f for f in os.listdir(path_to_data_folder)\n",
    "               if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "# Process images in batches\n",
    "all_features = []\n",
    "image_names = []\n",
    "\n",
    "total_start = time.time()\n",
    "\n",
    "for i in tqdm(range(0, len(image_files), BATCH_SIZE)):\n",
    "    batch_files = image_files[i:i+BATCH_SIZE]\n",
    "    batch_paths = [os.path.join(path_to_data_folder, f) for f in batch_files]\n",
    "\n",
    "    batch_features = get_batch_features(batch_paths)\n",
    "\n",
    "    all_features.extend(batch_features.cpu().numpy())\n",
    "    image_names.extend(batch_files)\n",
    "\n",
    "total_end = time.time()\n",
    "total_time = total_end - total_start\n",
    "\n",
    "# Convert to numpy array\n",
    "all_features = np.array(all_features)\n",
    "\n",
    "# Save features to CSV\n",
    "output_csv_path = \"dino_features_test.csv\"\n",
    "with open(output_csv_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    # Write header\n",
    "    writer.writerow(['image_name'] + [f'feature_{i}' for i in range(all_features.shape[1])])\n",
    "    # Write data\n",
    "    for name, features in zip(image_names, all_features):\n",
    "        writer.writerow([name] + features.tolist())\n",
    "\n",
    "print(f\"Features saved to {output_csv_path}\")\n",
    "print(f\"Processed {len(image_names)} images\")\n",
    "print(f\"Total processing time: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "476dc002-6052-4db1-b4e7-97f93327c833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/tamarbrand-perez/Desktop/Berkeley/w281 computer vision/final project/w281_final_project_plants\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b639ee22-73bd-4976-a958-9f6e7c29cc59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
